---
title: "A decade of code comment quality assessment: A systematic literature review"
collection: publications
permalink: /publication/2023-01-01-Rani22c-ADecadeOfCodeCommentQualityAssessment
excerpt: 'We present a Systematic Literature Review (SLR) of the last decade of research in SE.'
date: 2023-01-01
venue: 'Journal of Systems and Software'
paperurl: 'https://www.oscar.nierstrasz.org/files/publications/Rani22c-ADecadeOfCodeCommentQualityAssessment.pdf'
citation: 'Pooja Rani and Arianna Blasi and Nataliia Stulova and Sebastiano Panichella and Alessandra Gorla and Oscar Nierstrasz, A decade of code comment quality assessment: A systematic literature review, Journal of Systems and Software, 195, January 2023.'
---

[PDF download](https://www.oscar.nierstrasz.org/files/publications/Rani22c-ADecadeOfCodeCommentQualityAssessment.pdf)
| [SCG bib citation](https://scg.unibe.ch/scgbib/?query=Rani22c&filter=Year)

# Abstract
Code comments are important artifacts in software systems
and play a paramount role in many software engineering (SE) tasks related to maintenance and program
comprehension. However, while it is widely accepted that high quality matters in code comments just
as it matters in source code, assessing comment quality in practice is still an open problem. First
and foremost, there is no unique definition of quality when it comes to evaluating code comments.
The few existing studies on this topic rather focus on specific attributes of quality that can be
easily quantified and measured. Existing techniques and corresponding tools may also focus on
comments bound to a specific programming language, and may only deal with comments with specific
scopes and clear goals (e.g., Javadoc comments at the method level, or in-body comments describing
TODOs to be addressed). In this paper, we present a Systematic Literature Review (SLR) of the last
decade of research in SE to answer the following research questions: (i) What types of comments do
researchers focus on when assessing comment quality? (ii) What quality attributes (QAs) do they
consider? (iii) Which tools and techniques do they use to assess comment quality?, and (iv) How do
they evaluate their studies on comment quality assessment in general? Our evaluation, based on the
analysis of 2353 papers and the actual review of 47 relevant ones, shows that (i) most studies and
techniques focus on comments in Java code, thus may not be generalizable to other languages, and
(ii) the analyzed studies focus on four main QAs of a total of 21 QAs identified in the literature,
with a clear predominance of checking consistency between comments and the code. We observe that
researchers rely on manual assessment and specific heuristics rather than the automated assessment
of the comment quality attributes, with evaluations often involving surveys of students and the
authors of the original studies but rarely professional developers.
